{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMT for English to Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wow!', 'help!', 'jump.', 'jump.', 'jump.', 'hello!', 'hello!', 'cheers!', 'cheers!', 'got it?']\n",
      "['वाह!', 'बचाओ!', 'उछलो.', 'कूदो.', 'छलांग.', 'नमस्ते।', 'नमस्कार।', 'वाह-वाह!', 'चियर्स!', 'समझे कि नहीं?']\n",
      "\n",
      "length of english sentences: 2867\n",
      "length of hindi sentences: 2867\n"
     ]
    }
   ],
   "source": [
    "english = []\n",
    "hindi = []\n",
    "with open(os.path.join('hin-eng', 'hin.txt')) as f:\n",
    "    for line in f:\n",
    "        eng, hin = line.split('\\t')\n",
    "        # hindi is having a newline character '\\n' at the end\n",
    "        hin = hin[:-1]\n",
    "        english.append(eng.lower())\n",
    "        hindi.append(hin)\n",
    "        \n",
    "    assert len(english) == len(hindi)\n",
    "    \n",
    "    print(english[:10])\n",
    "    print(hindi[:10])\n",
    "    \n",
    "    print('\\nlength of english sentences:', len(english))\n",
    "    print('length of hindi sentences:', len(hindi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the length of unique sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2660\n",
      "2788\n"
     ]
    }
   ],
   "source": [
    "unique_eng_sentences = set(english)\n",
    "unique_hin_sentences = set(hindi)\n",
    "\n",
    "print(len(unique_eng_sentences))\n",
    "print(len(unique_hin_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique english words : 3307\n",
      "unique hindi words: 3156\n",
      "\n",
      "english dict:\n",
      " [('old', 3), ('map.', 4), ('is!', 6), ('beast.', 2209), ('meeting', 1677), ('abroad.', 5), ('heat.', 2373), ('loyalty.', 7), ('me.', 8), ('last,', 9)]\n",
      "\n",
      "reverse english dict:\n",
      " [(1, '<sos>'), (2, '<eos>'), (3, '<PAD>'), (4, 'map.'), (5, 'abroad.'), (6, 'is!'), (7, 'loyalty.'), (8, 'me.'), (9, 'last,'), (10, 'crash')]\n",
      "\n",
      "hindi dict:\n",
      " [('ब्रॅड', 3), ('बोर', 4), ('मुट्ठीभर', 8), ('बाँटा।', 7), ('दिल', 5), ('हाथ-पैर', 9), ('नाश्ता', 10), ('चली', 11), ('उपयोगी', 12), ('निधन', 13)]\n",
      "\n",
      "reverse hindi dict:\n",
      " [(1, '<sos>'), (2, '<eos>'), (3, '<PAD>'), (4, 'बोर'), (5, 'दिल'), (6, 'किसान'), (7, 'बाँटा।'), (8, 'मुट्ठीभर'), (9, 'हाथ-पैर'), (10, 'नाश्ता')]\n"
     ]
    }
   ],
   "source": [
    "eng_words = []\n",
    "hin_words = []\n",
    "\n",
    "for sent in english:\n",
    "    sentence = sent.split(' ')\n",
    "    eng_words.extend(sentence)\n",
    "\n",
    "unq_eng_words = set(eng_words)\n",
    "print('unique english words :', len(unq_eng_words))\n",
    "\n",
    "for sent in hindi:\n",
    "    sentence = sent.split(' ')\n",
    "    hin_words.extend(sentence)\n",
    "\n",
    "unq_hin_words = set(hin_words)\n",
    "print('unique hindi words:', len(unq_hin_words))\n",
    "\n",
    "eng_dict = {'<sos>':1, '<eos>':2, '<PAD>':3}\n",
    "\n",
    "for wrd in unq_eng_words:\n",
    "    eng_dict[wrd] = len(eng_dict)\n",
    "rev_eng_dict = dict(zip(eng_dict.values(), eng_dict.keys()))\n",
    "\n",
    "print('\\nenglish dict:\\n', list(eng_dict.items())[0:10])\n",
    "print('\\nreverse english dict:\\n', list(rev_eng_dict.items())[0:10])\n",
    "\n",
    "hin_dict = {'<sos>':1, '<eos>':2, '<PAD>':3}\n",
    "\n",
    "for wrd in unq_hin_words:\n",
    "    hin_dict[wrd] = len(hin_dict)\n",
    "    \n",
    "rev_hin_dict = dict(zip(hin_dict.values(), hin_dict.keys()))\n",
    "\n",
    "print('\\nhindi dict:\\n', list(hin_dict.items())[0:10])\n",
    "print('\\nreverse hindi dict:\\n', list(rev_hin_dict.items())[0:10])\n",
    "\n",
    "eng_vocab_size = len(unq_eng_words)\n",
    "hin_vocab_size = len(unq_hin_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of english train: 2817\n",
      "length of hindi train: 2817\n",
      "length of english test: 50\n",
      "length of hindi test: 50\n",
      "\n",
      "eng: is it necessary for me to attend the party?\n",
      "hin: मेरा पार्टी में आना ज़रूरी है क्या?\n",
      "\n",
      "eng: tokyo is a very big city.\n",
      "hin: टोक्यो बहुत बड़ा शहर है।\n",
      "\n",
      "eng: she has long arms and legs.\n",
      "hin: उसके हाथ-पैर लम्बे हैं।\n",
      "\n",
      "eng: they have demanded that all copies of the book be destroyed.\n",
      "hin: उन्होंने मांग करी है कि इस किताब की सारी कॉपियाँ नष्ट कर दीं जाएं।\n",
      "\n",
      "eng: he was my dear friend.\n",
      "hin: वह मेरा अच्छा दोस्त था।\n",
      "\n",
      "eng: that's my fault.\n",
      "hin: वह मेरी गलती है।\n",
      "\n",
      "eng: i was able to answer all the questions.\n",
      "hin: मैं सारे सवालों का जवाब दे पाया था।\n",
      "\n",
      "eng: he did the work on his own.\n",
      "hin: उसने काम अपने-आप किया।\n",
      "\n",
      "eng: perfect!\n",
      "hin: उत्तम!\n",
      "\n",
      "eng: i hope that it rains tomorrow.\n",
      "hin: काश कल बारिश हो जाए।\n"
     ]
    }
   ],
   "source": [
    "test_ind = [random.randint(0, len(english)) for i in range(50)]\n",
    "\n",
    "english_train = [english[i] for i in range(len(english)) if i not in test_ind]\n",
    "hindi_train = [hindi[i] for i in range(len(hindi)) if i not in test_ind]\n",
    "\n",
    "english_test = [english[i] for i in test_ind]\n",
    "hindi_test = [hindi[i] for i in test_ind]\n",
    "\n",
    "print('length of english train:', len(english_train))\n",
    "print('length of hindi train:', len(hindi_train))\n",
    "\n",
    "print('length of english test:', len(english_test))\n",
    "print('length of hindi test:', len(hindi_test))\n",
    "\n",
    "for i in range(10):\n",
    "    print('\\neng:', english_test[i])\n",
    "    print('hin:', hindi_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max no. of words in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of english sentence: 22\n",
      "max length of hindi sentence: 25\n"
     ]
    }
   ],
   "source": [
    "en = []\n",
    "hi = []\n",
    "\n",
    "for sent in english:\n",
    "    sentence = sent.split(' ')\n",
    "    en.append(sentence)\n",
    "\n",
    "for sent in hindi:\n",
    "    sentence = sent.split(' ')\n",
    "    hi.append(sentence)\n",
    "    \n",
    "print('max length of english sentence:', len(max(en, key = len)))\n",
    "print('max length of hindi sentence:', len(max(hi, key = len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting from words to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# +2 for <sos> and <eos>\n",
    "eng_max_len = len(max(en, key = len)) + 2\n",
    "hin_max_len = len(max(hi, key = len)) + 2\n",
    "\n",
    "train_input = []\n",
    "train_output = []\n",
    "\n",
    "for eng_sent, hin_sent in zip(english_train, hindi_train):\n",
    "    \n",
    "    numeric_eng_sent = [eng_dict['<sos>']]\n",
    "    numeric_hin_sent = [hin_dict['<sos>']]\n",
    "    #print(numeric_eng_sent)\n",
    "    \n",
    "    for wrds in eng_sent.split():\n",
    "        numeric_eng_sent.append(eng_dict[wrds])\n",
    "    \n",
    "    for wrds in hin_sent.split():\n",
    "        numeric_hin_sent.append(hin_dict[wrds])\n",
    "    \n",
    "    if len(numeric_eng_sent) < eng_max_len:\n",
    "        \n",
    "        [numeric_eng_sent.append(eng_dict['<PAD>']) \n",
    "                            for i in range(eng_max_len - len(numeric_eng_sent) - 1)]\n",
    "        \n",
    "        numeric_eng_sent.append(eng_dict['<eos>'])\n",
    "    \n",
    "    train_input.append(numeric_eng_sent)\n",
    "    \n",
    "    if len(numeric_hin_sent) < hin_max_len:\n",
    "        \n",
    "        [numeric_hin_sent.append(hin_dict['<PAD>']) \n",
    "                            for i in range(hin_max_len - len(numeric_hin_sent) - 1)]\n",
    "        numeric_hin_sent.append(hin_dict['<eos>'])\n",
    "    \n",
    "    train_output.append(numeric_hin_sent)\n",
    "\n",
    "\n",
    "test_input = []\n",
    "test_output = []\n",
    "\n",
    "for eng_sent, hin_sent in zip(english_test, hindi_test):\n",
    "    \n",
    "    numeric_eng_sent = [eng_dict['<sos>']]\n",
    "    numeric_hin_sent = [hin_dict['<sos>']]\n",
    "    #print(numeric_eng_sent)\n",
    "    \n",
    "    for wrds in eng_sent.split():\n",
    "        numeric_eng_sent.append(eng_dict[wrds])\n",
    "    \n",
    "    for wrds in hin_sent.split():\n",
    "        numeric_hin_sent.append(hin_dict[wrds])\n",
    "    \n",
    "    if len(numeric_eng_sent) < eng_max_len:\n",
    "        \n",
    "        [numeric_eng_sent.append(eng_dict['<PAD>']) \n",
    "                            for i in range(eng_max_len - len(numeric_eng_sent) - 1)]\n",
    "        \n",
    "        numeric_eng_sent.append(eng_dict['<eos>'])\n",
    "    \n",
    "    test_input.append(numeric_eng_sent)\n",
    "    \n",
    "    if len(numeric_hin_sent) < hin_max_len:\n",
    "        \n",
    "        [numeric_hin_sent.append(hin_dict['<PAD>']) \n",
    "                            for i in range(hin_max_len - len(numeric_hin_sent) - 1)]\n",
    "        numeric_hin_sent.append(hin_dict['<eos>'])\n",
    "    \n",
    "    test_output.append(numeric_hin_sent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if every thing is working fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "numeric and word sequence:\n",
      " [1, 2672, 2416, 3020, 220, 2299, 2192, 587, 2586, 1461, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2]\n",
      "['<sos>', 'is', 'it', 'necessary', 'for', 'me', 'to', 'attend', 'the', 'party?', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<eos>']\n",
      "\n",
      "numeric and word sequence:\n",
      " [1, 1971, 2672, 1820, 2300, 1262, 25, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2]\n",
      "['<sos>', 'tokyo', 'is', 'a', 'very', 'big', 'city.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<eos>']\n",
      "\n",
      "numeric and word sequence:\n",
      " [1, 862, 1914, 2622, 2135, 348, 2330, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2]\n",
      "['<sos>', 'she', 'has', 'long', 'arms', 'and', 'legs.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<eos>']\n",
      "\n",
      "numeric and word sequence:\n",
      " [1, 1224, 1961, 323, 1055, 2739, 889, 2744, 2586, 690, 790, 1201, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2]\n",
      "['<sos>', 'they', 'have', 'demanded', 'that', 'all', 'copies', 'of', 'the', 'book', 'be', 'destroyed.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<eos>']\n",
      "\n",
      "numeric and word sequence:\n",
      " [1, 1623, 162, 1306, 69, 1246, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2]\n",
      "['<sos>', 'he', 'was', 'my', 'dear', 'friend.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<eos>']\n",
      "\n",
      "numeric and word sequence:\n",
      " [1, 1789, 1306, 1813, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2]\n",
      "['<sos>', \"that's\", 'my', 'fault.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<eos>']\n",
      "\n",
      "numeric and word sequence:\n",
      " [1, 1594, 162, 282, 2192, 2506, 2739, 2586, 49, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2]\n",
      "['<sos>', 'i', 'was', 'able', 'to', 'answer', 'all', 'the', 'questions.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<eos>']\n",
      "\n",
      "numeric and word sequence:\n",
      " [1, 1623, 183, 2586, 1898, 119, 2958, 3081, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2]\n",
      "['<sos>', 'he', 'did', 'the', 'work', 'on', 'his', 'own.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<eos>']\n",
      "\n",
      "numeric and word sequence:\n",
      " [1, 741, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2]\n",
      "['<sos>', 'perfect!', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<eos>']\n",
      "\n",
      "numeric and word sequence:\n",
      " [1, 1594, 477, 1055, 2416, 2339, 2989, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2]\n",
      "['<sos>', 'i', 'hope', 'that', 'it', 'rains', 'tomorrow.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "wrd_sent = []\n",
    "for i in range(10):\n",
    "    num_sent = test_input[i]\n",
    "    print('\\nnumeric and word sequence:\\n', num_sent)\n",
    "    for j in range(24):\n",
    "        wrd_sent.append(rev_eng_dict[num_sent[j]])\n",
    "        \n",
    "    print(wrd_sent)\n",
    "    \n",
    "    wrd_sent = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings using skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['got', 'it?'],\n",
       " ['it?', 'got'],\n",
       " [\"i'm\", 'ok.'],\n",
       " ['ok.', \"i'm\"],\n",
       " ['come', 'in.'],\n",
       " ['in.', 'come'],\n",
       " ['get', 'out!'],\n",
       " ['out!', 'get'],\n",
       " ['go', 'away!'],\n",
       " ['away!', 'go']]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "embedding_size = 128\n",
    "\n",
    "sentences = []\n",
    "window_size = 1\n",
    "data = []\n",
    "\n",
    "num_sampled = 20\n",
    "\n",
    "for sent in english:\n",
    "    sentence = sent.split(' ')\n",
    "    sentences.append(sentence)\n",
    "    \n",
    "\n",
    "\n",
    "for sent in sentences:\n",
    "    for ind, wrd in enumerate(sent):\n",
    "        for cont_wrd in sent[max(ind - window_size, 0) : min(ind + window_size, len(sent)) + 1]:\n",
    "            if wrd not in cont_wrd:\n",
    "                data.append([wrd, cont_wrd])\n",
    "                \n",
    "data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data train: [1318, 642, 2062, 869, 3283, 1139, 1772, 1830, 1316, 2399]\n",
      "data label: [642, 1318, 869, 2062, 1139, 3283, 1830, 1772, 2399, 1316]\n"
     ]
    }
   ],
   "source": [
    "data_train = []\n",
    "data_label = []\n",
    "\n",
    "for inp in data:\n",
    "    \n",
    "    data_train.append(eng_dict[inp[0]])\n",
    "    data_label.append(eng_dict[inp[1]])\n",
    "\n",
    "print('data train: {}' .format(data_train[0:10]))\n",
    "print('data label: {}' .format(data_label[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define i/p and o/p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "train_dataset = tf.placeholder(tf.int32, shape = [batch_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape = [batch_size, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters and Other Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform([eng_vocab_size, embedding_size], -1.0, 1.0))\n",
    "\n",
    "softmax_weights = tf.Variable(tf.truncated_normal([eng_vocab_size, embedding_size],\n",
    "                                stddev=0.5 / math.sqrt(embedding_size))\n",
    "                                )\n",
    "softmax_biases = tf.Variable(tf.random_uniform([eng_vocab_size],0.0,0.01))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/monu/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(\n",
    "                    weights=softmax_weights, biases=softmax_biases, inputs=embed,\n",
    "                    labels=train_labels, num_sampled=num_sampled, num_classes=eng_vocab_size)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the skip gram algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (64,) for Tensor 'Placeholder_1:0', which has shape '(64, 1)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-a9b2e3c5298f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_label\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0maverage_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1109\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1111\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1112\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (64,) for Tensor 'Placeholder_1:0', which has shape '(64, 1)'"
     ]
    }
   ],
   "source": [
    "num_steps = 10000\n",
    "skip_losses = []\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer()\n",
    "    average_loss = 0\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        for batch_idx in range(len(data_train) // batch_size):\n",
    "            \n",
    "            batch_data = data_train[batch_size*batch_idx : batch_size*batch_idx + batch_size]\n",
    "            batch_label = data_label[batch_size*batch_idx : batch_size*batch_idx + batch_size]\n",
    "            \n",
    "            feed_dict = {train_dataset: batch_data, train_labels: batch_label}\n",
    "            _, l = session.run([optimizer, loss], feed_dict = feed_dict)\n",
    "            \n",
    "            average_loss += l\n",
    "            \n",
    "            if (step + 1) % 200 == 0:\n",
    "                if step > 0:\n",
    "                    average_loss / 2000\n",
    "                skip_losses.append(average_loss)\n",
    "                \n",
    "                print('Average loss at step %d: %f' % (step+1, average_loss))\n",
    "                average_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1318,\n",
       " 642,\n",
       " 2062,\n",
       " 869,\n",
       " 3283,\n",
       " 1139,\n",
       " 1772,\n",
       " 1830,\n",
       " 1316,\n",
       " 2399,\n",
       " 1961,\n",
       " 2563,\n",
       " 1961,\n",
       " 2563,\n",
       " 1961,\n",
       " 2563,\n",
       " 1594,\n",
       " 3198,\n",
       " 1594,\n",
       " 3198,\n",
       " 360,\n",
       " 2979,\n",
       " 2062,\n",
       " 738,\n",
       " 2062,\n",
       " 260,\n",
       " 828,\n",
       " 538,\n",
       " 2506,\n",
       " 8,\n",
       " 3171,\n",
       " 2487,\n",
       " 3094,\n",
       " 8,\n",
       " 909,\n",
       " 1594,\n",
       " 3184,\n",
       " 3184,\n",
       " 426,\n",
       " 1594,\n",
       " 2698,\n",
       " 2062,\n",
       " 1640,\n",
       " 2062,\n",
       " 1908,\n",
       " 2062,\n",
       " 34,\n",
       " 1921,\n",
       " 835,\n",
       " 581,\n",
       " 2379,\n",
       " 581,\n",
       " 2379,\n",
       " 581,\n",
       " 2379,\n",
       " 581,\n",
       " 2379,\n",
       " 3171,\n",
       " 883,\n",
       " 3283,\n",
       " 119,\n",
       " 119,\n",
       " 1139,\n",
       " 1326]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_conv(data_point, vocab_size):\n",
    "    vec = np.zeros(vocab_size)\n",
    "    vec[data_point] = 1\n",
    "    return vec\n",
    "\n",
    "train_data = []\n",
    "train_lables = []\n",
    "\n",
    "for dat in data:\n",
    "    \n",
    "    train_data.append(one_hot_conv(eng_dict[dat[0]], eng_vocab_size + 3))\n",
    "    train_labels.append(one_hot_conv(eng_dict[dat[1]], eng_vocab_size + 3))\n",
    "    \n",
    "train_data = np.asarray(train_data)\n",
    "train_labels = np.asarray(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "train_data = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
